{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TGEbx4dCRZ7R"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from dataset import MNIST_Moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZyUfMc1RdK8",
    "outputId": "db6b1739-6191-4290-c7dd-62e1979bf585"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_set = MNIST_Moving(root='.data/mnist', train=True, download=True)\n",
    "test_set = MNIST_Moving(root='.data/mnist', train=False, download=True)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 \n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tIFXGuppRjjv",
    "outputId": "d3c4c975-9a3c-42f1-b16e-72d51fd65b7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 20, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "input = next(iter(train_loader))\n",
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "goNhIrWBRoeM",
    "outputId": "c0c4f681-7db9-4de8-d177-7cfb6fecd4e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total trainning batch number: 90\n",
      "==>>> total testing batch number: 10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25336/4024166980.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'==>>> total testing batch number: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_target\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--- Sample'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input:  '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "print('==>>> total trainning batch number: {}'.format(len(train_loader)))\n",
    "print('==>>> total testing batch number: {}'.format(len(test_loader)))\n",
    "\n",
    "for seq, seq_target in train_loader:\n",
    "    print('--- Sample')\n",
    "    print('Input:  ', seq.shape)\n",
    "    print('Target: ', seq_target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "2nwN-a5kRvWU"
   },
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias,mode=\"zeros\"):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "        self.mode = mode\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "        \n",
    "       \n",
    "        \n",
    "\n",
    "    def forward(self, x, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        x = x.to(device)\n",
    "        h_cur = h_cur.to(device)\n",
    "        # print(x.size())\n",
    "        # print(h_cur.size())\n",
    "        concat_input_hcur = torch.cat([x, h_cur], dim=1) \n",
    "        concat_input_hcur = concat_input_hcur.to(device)\n",
    "\n",
    "        concat_input_hcur_conv = self.conv(concat_input_hcur)\n",
    "        concat_input_hcur_conv = concat_input_hcur_conv.to(device)\n",
    "\n",
    "        cc_input_gate, cc_forget_gate, cc_output_gate, cc_output = torch.split(concat_input_hcur_conv, self.hidden_dim, dim=1)\n",
    "        # print(\"cci\",cc_input_gate.shape)\n",
    "        # print(\"ccf\",cc_forget_gate.shape)\n",
    "        # print(\"ccog\",cc_output_gate.shape)\n",
    "        # print(\"cco\",cc_output.shape)\n",
    "        # print(\"cccur\",c_cur.shape)\n",
    "        # print(\"wci\",self.W_ci.shape)\n",
    "        \n",
    "        input_gate = torch.sigmoid(cc_input_gate +  c_cur)\n",
    "\n",
    "        forget_gate = torch.sigmoid(cc_forget_gate +  c_cur)\n",
    "\n",
    "        output = torch.tanh(cc_output)\n",
    "\n",
    "        c_next = forget_gate * c_cur + input_gate * output\n",
    "\n",
    "        output_gate = torch.sigmoid(cc_output_gate +  c_next)\n",
    "\n",
    "        h_next = output * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_state(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        \"\"\" Initializing hidden and cell state \"\"\"\n",
    "        if(self.mode == \"zeros\"):\n",
    "            h = torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
    "            c = torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
    "        elif(self.mode == \"random\"):\n",
    "            h = torch.randn(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
    "            c = torch.randn(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
    "        elif(self.mode == \"learned\"):\n",
    "            h = self.learned_h.repeat(batch_size, 1, height, width, device=self.conv.weight.device)\n",
    "            c = self.learned_c.repeat(batch_size, 1, height, width, device=self.conv.weight.device)\n",
    "        \n",
    "        return h, c\n",
    "\n",
    "        \n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    \n",
    "    \"\"\" \n",
    "    Custom LSTM for images. Batches of images are fed to a Conv LSTM\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "    input_dim: integer\n",
    "        Number of channels of the input.\n",
    "    hidden_dim: integer\n",
    "        dimensionality of the states in the cell\n",
    "    kernel_size: tuple\n",
    "        size of the kernel for convolutions\n",
    "    num_layers: integer\n",
    "        number of stacked LSTMS\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        \n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "       \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        conv_lstms  = []\n",
    "        # iterating over no of layers\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            conv_lstms.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.conv_lstms = nn.ModuleList(conv_lstms)\n",
    "\n",
    "    def forward(self, x, hidden_state=None):\n",
    "       \n",
    "\n",
    "        x=x.unsqueeze(dim=1)\n",
    "        b, _, _, h, w = x.size()\n",
    "\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            hidden_state = self._init_hidden(batch_size=b,\n",
    "                                             image_size=(h, w))\n",
    "\n",
    "        \n",
    "        cur_layer_input = x\n",
    "        output_list = []\n",
    "        x_len = x.size(1)\n",
    "        \n",
    "\n",
    "        # iterating over no of layers\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            h, c = hidden_state[i]\n",
    "            each_layer_output = []\n",
    "            # iterating over sequence length\n",
    "\n",
    "            for t in range(x_len):\n",
    "                h, c = self.conv_lstms[i](x=cur_layer_input[:, t, :, :, :],cur_state=[h, c])\n",
    "                each_layer_output.append(h)\n",
    "\n",
    "            stacked_layer_output = torch.stack(each_layer_output, dim=1)\n",
    "            cur_layer_input = stacked_layer_output\n",
    "\n",
    "            output_list.append(stacked_layer_output)\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            output_list = output_list[-1:]\n",
    "\n",
    "        batch_shape = output_list[-1].shape[0]\n",
    "\n",
    "        return torch.stack(each_layer_output), (h, c)\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.conv_lstms[i].init_state(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GVNlYWFURsDw",
    "outputId": "56ff5559-267c-4b76-ed9e-4b9a3a7db671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "0wNXXXeoSOjw"
   },
   "outputs": [],
   "source": [
    "class FirstLayerEncBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        # Add BN here \n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3,stride=1,padding=(1, 1))\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3,stride=1,padding=(1, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "class FirstEncoder(nn.Module):\n",
    "    def __init__(self, chs):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList([FirstLayerEncBlock(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "        self.pool       = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        layers = []\n",
    "        print(\"Staring encoding process\")\n",
    "        print(\"Input to the encoder\", x.shape)\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            x = self.pool(x)\n",
    "            print(\"Output after block\")\n",
    "            print(x.shape)\n",
    "            layers.append(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "F29_Ra-xViBE"
   },
   "outputs": [],
   "source": [
    "class FirstLayerDecBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(in_ch, out_ch, 3,stride=2,padding=(1, 1))\n",
    "        self.conv2 = nn.ConvTranspose2d(out_ch, out_ch, 2,stride=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv2((self.conv1(x)))\n",
    "\n",
    "class FirstDecoder(nn.Module):\n",
    "    def __init__(self, chs):\n",
    "        super().__init__()\n",
    "        self.dec_blocks = nn.ModuleList([FirstLayerDecBlock(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        layers = []\n",
    "        print(\"Starting decoding step\")\n",
    "        print(\"Input shape\", x.shape)\n",
    "\n",
    "        for block in self.dec_blocks:\n",
    "            x = block(x)\n",
    "            print(\"Intermediate shape\", x.shape)   \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xf66OChmUZ8D",
    "outputId": "fb7a2b52-b526-4305-ac4f-6aa632172d9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staring encoding process\n",
      "Input to the encoder torch.Size([100, 1, 64, 64])\n",
      "Output after block\n",
      "torch.Size([100, 16, 32, 32])\n",
      "Output after block\n",
      "torch.Size([100, 64, 16, 16])\n",
      "Correct step\n"
     ]
    }
   ],
   "source": [
    "enc_chs=(1,16,64)\n",
    "e= FirstEncoder(enc_chs)\n",
    "e.to(device)\n",
    "x = torch.randn((100,1, 64, 64))\n",
    "y1= e(input1[:,9:,:,:].float())\n",
    "print(\"Correct step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "0SzulpM5WOTa"
   },
   "outputs": [],
   "source": [
    "image_size = (28,28)\n",
    "output_label_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "6wJdhuHYWQca"
   },
   "outputs": [],
   "source": [
    "conv_model= ConvLSTM(input_dim= 64, hidden_dim = 64, kernel_size = (5,5), num_layers= 1)\n",
    "if torch.cuda.is_available():\n",
    "    conv_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtFtGIrVWS7i",
    "outputId": "e31ec78e-37ce-4ce1-95fd-fc0781ace0d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLSTM(\n",
      "  (conv_lstms): ModuleList(\n",
      "    (0): ConvLSTMCell(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(conv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4wUTgxVWTnr",
    "outputId": "28a1002c-7871-4793-ef14-da21c33ec974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 64, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "c=conv_model(y1)\n",
    "print(c[1][0].size())\n",
    "convlstm_out=c[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2xEm2s-FWwhl",
    "outputId": "efffb4f6-b3f3-479d-ba20-8fdc5f75c44f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting decoding step\n",
      "Input shape torch.Size([100, 64, 16, 16])\n",
      "Intermediate shape torch.Size([100, 16, 32, 32])\n",
      "Intermediate shape torch.Size([100, 1, 64, 64])\n",
      "correct step\n"
     ]
    }
   ],
   "source": [
    "dec_chs=(64,16,1)\n",
    "d= FirstDecoder(dec_chs)\n",
    "d.to(device)\n",
    "\n",
    "cv=d(convlstm_out)\n",
    "print(\"correct step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTkgLpGkbZY9",
    "outputId": "59b10228-441b-4abc-de1a-60bce1f86a32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staring encoding process\n",
      "Input to the encoder torch.Size([100, 64, 16, 16])\n",
      "Output after block\n",
      "torch.Size([100, 128, 8, 8])\n",
      "torch.Size([100, 128, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "enc_chs1=(64,128)\n",
    "e1= FirstEncoder(enc_chs1)\n",
    "e1.to(device)\n",
    "y2= e1(y1)\n",
    "print(y2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "Kww3Qqpvdo8x"
   },
   "outputs": [],
   "source": [
    "conv_model1= ConvLSTM(input_dim= 128, hidden_dim = 128, kernel_size = (5,5), num_layers= 1)\n",
    "if torch.cuda.is_available():\n",
    "    conv_model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRsQiy4HcqbM",
    "outputId": "31b1e51e-961d-401a-d915-64f9bae0f136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 128, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "c1=conv_model1(y2)\n",
    "print(c1[1][0].size())\n",
    "convlstm_out1=c1[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fA93j2NFfwaP",
    "outputId": "9532e819-52df-4c56-9ffa-a4489d54e58b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "torch.Size([100, 128, 8, 8])\n",
      "torch.Size([100, 64, 16, 16])\n",
      "torch.Size([100, 64, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "dec_chs1=(128,64)\n",
    "d2= FirstDecoder(dec_chs1)\n",
    "d2.to(device)\n",
    "\n",
    "cv1=d2(convlstm_out1)\n",
    "print(cv1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jRDj1oTmgXWG",
    "outputId": "64a97513-0bfb-44f0-9b64-b09188ebaa63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "torch.Size([100, 128, 8, 8])\n",
      "torch.Size([100, 256, 8, 8])\n",
      "torch.Size([100, 256, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "enc_chs2=(128,256)\n",
    "e2= FirstEncoder(enc_chs2)\n",
    "e2.to(device)\n",
    "y3= e2(y2)\n",
    "print(y3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "uDlod2kcgkTB"
   },
   "outputs": [],
   "source": [
    "conv_model2= ConvLSTM(input_dim= 256, hidden_dim = 256, kernel_size = (5,5), num_layers= 1)\n",
    "if torch.cuda.is_available():\n",
    "    conv_model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KqnhYf7gvAw",
    "outputId": "bc55ed0a-12fd-4e60-d6e1-39f6d12fbcdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 256, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "c2=conv_model2(y3)\n",
    "print(c2[1][0].size())\n",
    "convlstm_out2=c2[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJtRmC6hg5Xn",
    "outputId": "0587151c-be10-4500-cbd9-b00df1a6bf05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "torch.Size([100, 256, 4, 4])\n",
      "torch.Size([100, 128, 8, 8])\n",
      "torch.Size([100, 128, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "dec_chs2=(256,128)\n",
    "d3= FirstDecoder(dec_chs2)\n",
    "d3.to(device)\n",
    "\n",
    "cv2=d3(convlstm_out2)\n",
    "print(cv2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6qIboXekH2u"
   },
   "outputs": [],
   "source": [
    "class FirstLayerDecBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(in_ch, out_ch, 3,stride=2,padding=(1, 1))\n",
    "        self.conv2 = nn.ConvTranspose2d(out_ch, out_ch, 2,stride=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv2((self.conv1(x)))\n",
    "\n",
    "class FirstDecoder(nn.Module):\n",
    "    def __init__(self, chs):\n",
    "        super().__init__()\n",
    "        self.dec_blocks = nn.ModuleList([FirstLayerDecBlock(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        layers = []\n",
    "        print(\"hello\")\n",
    "        print(x.shape)\n",
    "        for block in self.dec_blocks:\n",
    "            x = block(x)\n",
    "            print(x.shape)\n",
    "            \n",
    "           \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Proj_architecture.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
