{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proj_Arch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kSIvJPQ3UhSc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from MovingMNIST import MovingMNIST"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_set = MovingMNIST(root='.data/mnist', train=True, download=True)\n",
        "test_set = MovingMNIST(root='.data/mnist', train=False, download=True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=train_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=test_set,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7eV7rdxm8Gl",
        "outputId": "2aea6cb8-10d5-4cb1-b02e-b860c81612b8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/tychovdo/MovingMNIST/raw/master/mnist_test_seq.npy.gz\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('==>>> total trainning batch number: {}'.format(len(train_loader)))\n",
        "print('==>>> total testing batch number: {}'.format(len(test_loader)))\n",
        "\n",
        "for seq, seq_target in train_loader:\n",
        "    print('--- Sample')\n",
        "    print('Input:  ', seq.shape)\n",
        "    print('Target: ', seq_target.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qow4Wz5nPlw",
        "outputId": "8a3764d6-ef37-48db-f714-ebe8021cb7d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==>>> total trainning batch number: 90\n",
            "==>>> total testing batch number: 10\n",
            "--- Sample\n",
            "Input:   torch.Size([100, 10, 64, 64])\n",
            "Target:  torch.Size([100, 10, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import io\n",
        "import imageio\n",
        "from ipywidgets import widgets, HBox"
      ],
      "metadata": {
        "id": "RV6NFMnKvDRO"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input, _ = next(iter(train_loader))\n",
        "\n",
        "# Reverse process before displaying\n",
        "input = input.cpu().numpy() * 255.0     \n",
        "\n",
        "for video in input.squeeze(1)[:3]:          # Loop over videos\n",
        "    with io.BytesIO() as gif:\n",
        "        imageio.mimsave(gif,video.astype(np.uint8),\"GIF\",fps=5)\n",
        "        display(HBox([widgets.Image(value=gif.getvalue())]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "IDKIp8Iau6YA",
        "outputId": "f13cffe6-b612-4a5c-ee45-d2abbd0e69ea"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-afa34ed16702>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m          \u001b[0;31m# Loop over videos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgif\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgif\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"GIF\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot select an axis to squeeze out which has size not equal to one"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyDiQ7Q2nz0n",
        "outputId": "c5b917c6-5b32-43bd-fa08-e89af4fe1c53"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, bias,mode=\"zeros\"):\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
        "        self.bias = bias\n",
        "        self.mode = mode\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
        "                              out_channels=4 * self.hidden_dim,\n",
        "                              kernel_size=self.kernel_size,\n",
        "                              padding=self.padding,\n",
        "                              bias=self.bias)\n",
        "        \n",
        "        self.W_ci = nn.Parameter(torch.zeros(1, self.hidden_dim, 16, 16))\n",
        "        self.W_cf = nn.Parameter(torch.zeros(1, self.hidden_dim,  16, 16))\n",
        "        self.W_co = nn.Parameter(torch.zeros(1, self.hidden_dim,  16, 16))\n",
        "        \n",
        "\n",
        "    def forward(self, x, cur_state):\n",
        "        h_cur, c_cur = cur_state\n",
        "        x = x.to(device)\n",
        "        h_cur = h_cur.to(device)\n",
        "        # print(x.size())\n",
        "        # print(h_cur.size())\n",
        "        concat_input_hcur = torch.cat([x, h_cur], dim=1) \n",
        "        concat_input_hcur = concat_input_hcur.to(device)\n",
        "\n",
        "        concat_input_hcur_conv = self.conv(concat_input_hcur)\n",
        "        concat_input_hcur_conv = concat_input_hcur_conv.to(device)\n",
        "\n",
        "        cc_input_gate, cc_forget_gate, cc_output_gate, cc_output = torch.split(concat_input_hcur_conv, self.hidden_dim, dim=1)\n",
        "        # print(\"cci\",cc_input_gate.shape)\n",
        "        # print(\"ccf\",cc_forget_gate.shape)\n",
        "        # print(\"ccog\",cc_output_gate.shape)\n",
        "        # print(\"cco\",cc_output.shape)\n",
        "        # print(\"cccur\",c_cur.shape)\n",
        "        # print(\"wci\",self.W_ci.shape)\n",
        "        \n",
        "        input_gate = torch.sigmoid(cc_input_gate + self.W_ci * c_cur)\n",
        "\n",
        "        forget_gate = torch.sigmoid(cc_forget_gate + self.W_cf * c_cur)\n",
        "\n",
        "        output = torch.tanh(cc_output)\n",
        "\n",
        "        c_next = forget_gate * c_cur + input_gate * output\n",
        "\n",
        "        output_gate = torch.sigmoid(cc_output_gate + self.W_co * c_next)\n",
        "\n",
        "        h_next = output * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_state(self, batch_size, image_size):\n",
        "        height, width = image_size\n",
        "        \"\"\" Initializing hidden and cell state \"\"\"\n",
        "        if(self.mode == \"zeros\"):\n",
        "            h = torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
        "            c = torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
        "        elif(self.mode == \"random\"):\n",
        "            h = torch.randn(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
        "            c = torch.randn(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
        "        elif(self.mode == \"learned\"):\n",
        "            h = self.learned_h.repeat(batch_size, 1, height, width, device=self.conv.weight.device)\n",
        "            c = self.learned_c.repeat(batch_size, 1, height, width, device=self.conv.weight.device)\n",
        "        \n",
        "        return h, c\n",
        "\n",
        "        \n",
        "\n",
        "class ConvLSTM(nn.Module):\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "    \"\"\" \n",
        "    Custom LSTM for images. Batches of images are fed to a Conv LSTM\n",
        "    \n",
        "    Args:\n",
        "    -----\n",
        "    input_dim: integer\n",
        "        Number of channels of the input.\n",
        "    hidden_dim: integer\n",
        "        dimensionality of the states in the cell\n",
        "    kernel_size: tuple\n",
        "        size of the kernel for convolutions\n",
        "    num_layers: integer\n",
        "        number of stacked LSTMS\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,batch_first=False, bias=True, return_all_layers=False):\n",
        "        super(ConvLSTM, self).__init__()\n",
        "\n",
        "        \n",
        "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
        "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
        "       \n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.bias = bias\n",
        "        self.return_all_layers = return_all_layers\n",
        "        classifier_in_dim= (self.hidden_dim[0]*16*16)\n",
        "        classifier_output_dim = output_label_size\n",
        "\n",
        "        # FC-classifier\n",
        "        self.classifier = nn.Linear(classifier_in_dim, classifier_output_dim)\n",
        "\n",
        "        conv_lstms  = []\n",
        "        # iterating over no of layers\n",
        "        for i in range(0, self.num_layers):\n",
        "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
        "\n",
        "            conv_lstms.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
        "                                          hidden_dim=self.hidden_dim[i],\n",
        "                                          kernel_size=self.kernel_size[i],\n",
        "                                          bias=self.bias))\n",
        "\n",
        "        self.conv_lstms = nn.ModuleList(conv_lstms)\n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "       \n",
        "\n",
        "        x=x.unsqueeze(dim=1)\n",
        "        b, _, _, h, w = x.size()\n",
        "\n",
        "        if hidden_state is not None:\n",
        "            raise NotImplementedError()\n",
        "        else:\n",
        "            hidden_state = self._init_hidden(batch_size=b,\n",
        "                                             image_size=(h, w))\n",
        "\n",
        "        \n",
        "        cur_layer_input = x\n",
        "        output_list = []\n",
        "        x_len = x.size(1)\n",
        "        \n",
        "\n",
        "        # iterating over no of layers\n",
        "        for i in range(self.num_layers):\n",
        "\n",
        "            h, c = hidden_state[i]\n",
        "            each_layer_output = []\n",
        "            # iterating over sequence length\n",
        "\n",
        "            for t in range(x_len):\n",
        "                h, c = self.conv_lstms[i](x=cur_layer_input[:, t, :, :, :],cur_state=[h, c])\n",
        "                each_layer_output.append(h)\n",
        "\n",
        "            stacked_layer_output = torch.stack(each_layer_output, dim=1)\n",
        "            cur_layer_input = stacked_layer_output\n",
        "\n",
        "            output_list.append(stacked_layer_output)\n",
        "\n",
        "        if not self.return_all_layers:\n",
        "            output_list = output_list[-1:]\n",
        "\n",
        "        batch_shape = output_list[-1].shape[0]\n",
        "\n",
        "        # classifying\n",
        "        final_out= self.classifier(output_list[-1].view(batch_shape, - 1)) # feeding only output at last layer\n",
        "\n",
        "        return final_out\n",
        "\n",
        "    def _init_hidden(self, batch_size, image_size):\n",
        "        init_states = []\n",
        "        for i in range(self.num_layers):\n",
        "            init_states.append(self.conv_lstms[i].init_state(batch_size, image_size))\n",
        "        return init_states\n",
        "\n",
        "    @staticmethod\n",
        "    def _extend_for_multilayer(param, num_layers):\n",
        "        if not isinstance(param, list):\n",
        "            param = [param] * num_layers\n",
        "        return param"
      ],
      "metadata": {
        "id": "DlUeukX96Mwy"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 1,stride=1,padding=(3, 3))\n",
        "        self.relu  = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 1,stride=1,padding=(2, 2))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.conv2(self.relu(self.conv1(x)))\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, chs):\n",
        "        super().__init__()\n",
        "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
        "        self.pool       = nn.MaxPool2d(2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        layers = []\n",
        "        print(\"hello\")\n",
        "        for block in self.enc_blocks:\n",
        "            x = block(x)\n",
        "            print(x.shape)\n",
        "            layers.append(x)\n",
        "            x = self.pool(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, chs):\n",
        "        super().__init__()\n",
        "        self.chs         = chs\n",
        "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
        "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n",
        "        \n",
        "    def forward(self, x, encoder_features):\n",
        "        for i in range(len(self.chs)-1):\n",
        "            x        = self.upconvs[i](x)\n",
        "            enc_ftrs = self.crop(encoder_features[i], x)\n",
        "            x        = torch.cat([x, enc_ftrs], dim=1)\n",
        "            x        = self.dec_blocks[i](x)\n",
        "        return x\n",
        "    \n",
        "    def crop(self, enc_ftrs, x):\n",
        "        _, _, H, W = x.shape\n",
        "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
        "        return enc_ftrs\n",
        "\n",
        "\n",
        "class EncDecCNN(nn.Module):\n",
        "    def __init__(self, dec_chs=(16,32,64), enc_chs=(64,32,16), num_class=1, retain_dim=False, out_sz=(572,572)):\n",
        "        super().__init__()\n",
        "        self.encoder     = Encoder(enc_chs)\n",
        "        self.decoder     = Decoder(dec_chs)\n",
        "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
        "        self.retain_dim  = retain_dim\n",
        "        self.out_sz  = out_sz\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_ftrs = self.encoder(x)\n",
        "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
        "        out      = self.head(out)\n",
        "        if self.retain_dim:\n",
        "            out = F.interpolate(out, self.out_sz)\n",
        "        return out"
      ],
      "metadata": {
        "id": "9TDjp2nF1DOp"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_chs=(1,16,32,64)\n",
        "e= Encoder(enc_chs)\n",
        "x = torch.randn((100,1, 64, 64))\n",
        "y1= e(x)\n",
        "print(\"y1\",y1.size())"
      ],
      "metadata": {
        "id": "35QH2SvhIXen",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae3756c-3e6b-4aa2-f7bd-691c75393aea"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n",
            "torch.Size([100, 16, 74, 74])\n",
            "torch.Size([100, 32, 47, 47])\n",
            "torch.Size([100, 64, 33, 33])\n",
            "y1 torch.Size([100, 64, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = (28,28)\n",
        "output_label_size = 10"
      ],
      "metadata": {
        "id": "bi0FvXF5ogRm"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_model= ConvLSTM(input_dim= 64, hidden_dim = 64, kernel_size = (5,5), num_layers= 1)\n",
        "if torch.cuda.is_available():\n",
        "    conv_model.to(device)"
      ],
      "metadata": {
        "id": "VUq8wGpBoTBT"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conv_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRedWg9nr2QF",
        "outputId": "d95e026f-8e1d-4e10-da14-bf9488317109"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvLSTM(\n",
            "  (classifier): Linear(in_features=16384, out_features=10, bias=True)\n",
            "  (conv_lstms): ModuleList(\n",
            "    (0): ConvLSTMCell(\n",
            "      (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v3Kgi3oHsuaN"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c=conv_model(y1)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1GsmF2QokxB",
        "outputId": "d4446696-1d27-4e62-bb81-5314e00322e5"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0044, -0.0019,  0.0049,  0.0063, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0043,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0062, -0.0050, -0.0044,  0.0014, -0.0019,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0050, -0.0043,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0019,  0.0050,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0062, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0019,  0.0049,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0050,  0.0063, -0.0051, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0043,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0076],\n",
            "        [-0.0044, -0.0019,  0.0049,  0.0063, -0.0051, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0014, -0.0019,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0051, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0050,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0019,  0.0049,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0050,  0.0063, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0015, -0.0019,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0019,  0.0049,  0.0063, -0.0051, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0019,  0.0049,  0.0063, -0.0050, -0.0043,  0.0014, -0.0019,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0050,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0050,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0050,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0019,  0.0050,  0.0063, -0.0051, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0051, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0050,  0.0064, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0007,  0.0075],\n",
            "        [-0.0043, -0.0019,  0.0049,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0014, -0.0017,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0050,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0050,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0019,  0.0049,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0019,  0.0049,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0050, -0.0043,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0050,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0019,  0.0049,  0.0063, -0.0051, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0019,  0.0050,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0050,  0.0063, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0019,  0.0050,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0050,  0.0063, -0.0051, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0019,  0.0049,  0.0063, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0019,  0.0049,  0.0064, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0019,  0.0049,  0.0063, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0051, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0051, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0050,  0.0063, -0.0051, -0.0043,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0050,  0.0063, -0.0050, -0.0043,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0015, -0.0019,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0019,  0.0049,  0.0063, -0.0050, -0.0043,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0050,  0.0063, -0.0050, -0.0043,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0050,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0051, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0050, -0.0043,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0050,  0.0063, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0051, -0.0043,  0.0015, -0.0019,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0074],\n",
            "        [-0.0044, -0.0019,  0.0049,  0.0063, -0.0051, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0050,  0.0063, -0.0051, -0.0043,  0.0015, -0.0018,\n",
            "          0.0007,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0051, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0050,  0.0062, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0051, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0051, -0.0043,  0.0015, -0.0018,\n",
            "          0.0007,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0050,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0051, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0043,  0.0015, -0.0019,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0050, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0051, -0.0044,  0.0014, -0.0018,\n",
            "          0.0007,  0.0075],\n",
            "        [-0.0043, -0.0019,  0.0049,  0.0063, -0.0051, -0.0043,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0051, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0019,  0.0049,  0.0063, -0.0051, -0.0044,  0.0014, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0019,  0.0049,  0.0063, -0.0051, -0.0044,  0.0015, -0.0019,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0019,  0.0049,  0.0063, -0.0051, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0050,  0.0063, -0.0051, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075],\n",
            "        [-0.0044, -0.0019,  0.0049,  0.0063, -0.0050, -0.0044,  0.0014, -0.0018,\n",
            "          0.0005,  0.0075],\n",
            "        [-0.0043, -0.0020,  0.0049,  0.0063, -0.0050, -0.0044,  0.0015, -0.0018,\n",
            "          0.0006,  0.0075]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dec_chs=(64,32,16,1)\n",
        "d= Decoder(dec_chs)\n",
        "x = torch.randn((1,64, 16, 16))\n",
        "y= d(y1[::-1][0], y1[::-1][1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "6WeZpFZAjkyH",
        "outputId": "f8ee409b-5325-4f84-815e-03dfafcd30ea"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4543eb4fb98d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_chs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-0935e1fc6974>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_features)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mx\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupconvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0menc_ftrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mx\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_ftrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mx\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_blocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EDmodel=EncDecCNN(dec_chs=(64,32,16,1), enc_chs=(1,16,32,64), num_class=10, retain_dim=False)"
      ],
      "metadata": {
        "id": "HClhK1PS3LzZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(EDmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw5ITqsu302C",
        "outputId": "6db07b89-33d2-43da-9a3c-3ce2289d0382"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EncDecCNN(\n",
            "  (encoder): Encoder(\n",
            "    (enc_blocks): ModuleList(\n",
            "      (0): Block(\n",
            "        (conv1): Conv2d(1, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU()\n",
            "        (conv2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (1): Block(\n",
            "        (conv1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU()\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (2): Block(\n",
            "        (conv1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU()\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (upconvs): ModuleList(\n",
            "      (0): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (1): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (2): ConvTranspose2d(16, 1, kernel_size=(2, 2), stride=(2, 2))\n",
            "    )\n",
            "    (dec_blocks): ModuleList(\n",
            "      (0): Block(\n",
            "        (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU()\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (1): Block(\n",
            "        (conv1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU()\n",
            "        (conv2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (2): Block(\n",
            "        (conv1): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (relu): ReLU()\n",
            "        (conv2): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): Conv2d(1, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8eMAEAwElC4l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}