{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GQF1IAGSXBmo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from dataset import MNIST_Moving\n",
    "\n",
    "from encoder import Embedded_Encoder\n",
    "from decoder import Embedded_Decoder\n",
    "from conv_lstm import ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-frR5EzXMEn",
    "outputId": "fe3dc13b-5ea8-41c4-e192-2be2374fad22"
   },
   "outputs": [],
   "source": [
    "train_set = MNIST_Moving(root='.data/mnist', train=True, download=True)\n",
    "test_set = MNIST_Moving(root='.data/mnist', train=False, download=True)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 \n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZK5dQ8pHXOtH",
    "outputId": "6e74e2e0-c049-4a19-db61-b667f00403c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 20, 1, 64, 64])\n",
      "torch.Size([20, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "input= next(iter(train_loader))\n",
    "print(input.shape)\n",
    "vid_seq = input[0]\n",
    "print(vid_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cFJiQznKXRqw",
    "outputId": "e7671970-7f73-43f9-854c-ed74524ada21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FF-rceP8dWAe",
    "outputId": "ff9e1098-6011-48b3-ddf0-296d64c72c75"
   },
   "outputs": [],
   "source": [
    "# ModelEncoder = Embedded_Encoder(device)\n",
    "# print(\"The input is of the size\",vid_seq.shape)\n",
    "\n",
    "# encoderLayerOutputs=ModelEncoder(vid_seq.float().cuda())\n",
    "# #These features or outputs from each encoder can be then passed on to the convlstm \n",
    "\n",
    "# print(\"The outputs of each encoder in the embedded encoder:\")\n",
    "# for ftr in encoderLayerOutputs: print(ftr.shape)\n",
    "\n",
    "\n",
    "# ModelDecoder=Embedded_Decoder()\n",
    "# print(\"The input of the embedded decoder is the output from the last encoder in the embedded encoder\",encoderLayerOutputs[-1].shape)\n",
    "# decoderLayerOutputs=ModelDecoder(encoderLayerOutputs[-1])\n",
    "# print(\"The outputs of each encoder in the embedded decoder:\")\n",
    "# for ftr in decoderLayerOutputs: print(ftr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Y0jJdJC3nXBQ"
   },
   "outputs": [],
   "source": [
    "conv_model_FirstEncoder = ConvLSTM(input_dim= 64, hidden_dim = 64, kernel_size = (5,5), num_layers= 2)\n",
    "if torch.cuda.is_available():\n",
    "    conv_model_FirstEncoder.to(device)\n",
    "\n",
    "conv_model_SecondEncoder= ConvLSTM(input_dim= 128, hidden_dim = 128, kernel_size = (5,5), num_layers= 2)\n",
    "if torch.cuda.is_available():\n",
    "    conv_model_SecondEncoder.to(device)\n",
    "\n",
    "conv_model_ThirdEncoder= ConvLSTM(input_dim= 256, hidden_dim = 256, kernel_size = (5,5), num_layers= 2)\n",
    "if torch.cuda.is_available():\n",
    "    conv_model_ThirdEncoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pMTmv2H6nJxe"
   },
   "outputs": [],
   "source": [
    "abc = torch.rand((1, 64, 16, 16))\n",
    "abc_3 = torch.rand((1, 256, 4, 4))\n",
    "\n",
    "llm = conv_model_FirstEncoder(abc)\n",
    "llm_3 = conv_model_ThirdEncoder(abc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "def training(criterion):\n",
    "#     Seq of 20 images\n",
    "    input_seq = torch.rand(((20, 1, 64, 64)))\n",
    "    embd_model = Embedded_Encoder(device)\n",
    "    embds = embd_model(input_seq.float().cuda())\n",
    "    lstm_1, (h_1, c_1) = conv_model_FirstEncoder(embds[0])\n",
    "    lstm_2, (h_2, c_2) = conv_model_SecondEncoder(embds[1])\n",
    "    lstm_3, (h_3, c_3) = conv_model_ThirdEncoder(embds[2])\n",
    "    \n",
    "    print(lstm_1.shape)\n",
    "    print(lstm_2.shape)\n",
    "    print(lstm_3.shape)\n",
    "    decoder = Embedded_Decoder(device)\n",
    "    out_1 = decoder.firstDecoder(lstm_3.squeeze())\n",
    "    print(out_1.shape)\n",
    "    in_2 = torch.cat((out_1, lstm_2.squeeze()), 1)\n",
    "    out_2 = decoder.secondDecoder(in_2)\n",
    "    print(out_2.shape)\n",
    "    in_3 = torch.cat((out_2, lstm_1.squeeze()), 1)\n",
    "    out_3 = decoder.thirdDecoder(in_3)\n",
    "    print(out_3.shape)\n",
    "    \n",
    "    loss = criterion(input_seq[10:, :, :, :].to(device), out_3[10:, :, :, :])\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 64, 16, 16])\n",
      "torch.Size([1, 20, 128, 8, 8])\n",
      "torch.Size([1, 20, 256, 4, 4])\n",
      "torch.Size([20, 128, 8, 8])\n",
      "torch.Size([20, 64, 16, 16])\n",
      "torch.Size([20, 1, 64, 64])\n",
      "tensor(0.2782, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "training(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = torch.rand(((20, 1, 64, 64)))\n",
    "\n",
    "loss = criterion(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.firstDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Encoder_Decoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
