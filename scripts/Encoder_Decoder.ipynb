{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Encoder_Decoder.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GQF1IAGSXBmo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from dataset import MNIST_Moving"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_set = MNIST_Moving(root='.data/mnist', train=True, download=True)\n",
        "test_set = MNIST_Moving(root='.data/mnist', train=False, download=True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=train_set,\n",
        "                 batch_size=batch_size,\n",
        "                 \n",
        "                 shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=test_set,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-frR5EzXMEn",
        "outputId": "fe3dc13b-5ea8-41c4-e192-2be2374fad22"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/tychovdo/MovingMNIST/raw/master/mnist_test_seq.npy.gz\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input= next(iter(train_loader))\n",
        "print(input.shape)\n",
        "vid_seq = input[0]\n",
        "print(vid_seq.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK5dQ8pHXOtH",
        "outputId": "6e74e2e0-c049-4a19-db61-b667f00403c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 20, 1, 64, 64])\n",
            "torch.Size([20, 1, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFJiQznKXRqw",
        "outputId": "e7671970-7f73-43f9-854c-ed74524ada21"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3,stride=1,padding=(1, 1))\n",
        "        self.relu  = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3,stride=1,padding=(1, 1))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.conv2(self.relu(self.conv1(x)))\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, chs):\n",
        "        super().__init__()\n",
        "        self.enc_blocks = nn.ModuleList([EncBlock(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
        "        self.pool       = nn.MaxPool2d(2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        layers = []\n",
        "        # print(\"Layer shape\")\n",
        "        # print(x.shape)\n",
        "        for block in self.enc_blocks:\n",
        "            x = block(x)\n",
        "            # print(x.shape)\n",
        "            \n",
        "            x = self.pool(x)\n",
        "            layers.append(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "UQfyYhOBXU4R"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.ConvTranspose2d(in_ch, out_ch, 3,stride=2,padding=(1, 1))\n",
        "        self.conv2 = nn.ConvTranspose2d(out_ch, out_ch, 2,stride=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.conv2((self.conv1(x)))\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, chs):\n",
        "        super().__init__()\n",
        "        self.dec_blocks = nn.ModuleList([DecBlock(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        layers = []\n",
        "        # print(\"hello\")\n",
        "        # print(x.shape)\n",
        "        for block in self.dec_blocks:\n",
        "            x = block(x)\n",
        "            # print(x.shape)\n",
        "            layers.append(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "bVX_dBbuXmbo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedded_Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.firstEncoder= Encoder((1,16,64))\n",
        "      self.firstEncoder.to(device)\n",
        "      self.secondEncoder= Encoder((64,128))\n",
        "      self.secondEncoder.to(device)\n",
        "      self.thirdEncoder= Encoder((128,256))\n",
        "      self.thirdEncoder.to(device)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      #list to store the outputs from each encoders\n",
        "      encoder_outputs = []\n",
        "      x=self.firstEncoder(x)\n",
        "      encoder_outputs.append(x)\n",
        "      x=self.secondEncoder(x)\n",
        "      encoder_outputs.append(x)\n",
        "      x=self.thirdEncoder(x)\n",
        "      encoder_outputs.append(x)\n",
        "\n",
        "      \n",
        "      return encoder_outputs"
      ],
      "metadata": {
        "id": "6O0EPPt5Xssu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder implemented here is standalone. It does not take inputs from the conv lstm. Todo: Concatenate input from the convlstms with the ouputs from the corresponding encoders and pass them as inputs to each decoder. "
      ],
      "metadata": {
        "id": "5piDKjF2l6av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedded_Decoder(nn.Module):\n",
        "  \n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      #The decoder is in reverse here \n",
        "      self.firstDecoder= Decoder((256,128))\n",
        "      self.firstDecoder.to(device)\n",
        "      self.secondDecoder= Decoder((128,64))\n",
        "      self.secondDecoder.to(device)\n",
        "      self.thirdDecoder= Decoder((64,16,1))\n",
        "      self.thirdDecoder.to(device)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      #list to store the outputs from each decoders\n",
        "      decoder_outputs = []\n",
        "      x=self.firstDecoder(x)\n",
        "      decoder_outputs.append(x)\n",
        "      x=self.secondDecoder(x)\n",
        "      decoder_outputs.append(x)\n",
        "      x=self.thirdDecoder(x)\n",
        "      decoder_outputs.append(x)\n",
        "\n",
        "      \n",
        "      return decoder_outputs"
      ],
      "metadata": {
        "id": "PjuO1RgTbQnK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the input shape [20, 1, 64, 64] as conv2d has 4d weights. So to train we have to iterate through the batches and pass each video sequence to the encoder."
      ],
      "metadata": {
        "id": "6nhkfWyKmZ11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ModelEncoder=Embedded_Encoder()\n",
        "print(\"The input is of the size\",vid_seq.shape)\n",
        "encoderLayerOutputs=ModelEncoder(vid_seq.float().cuda())\n",
        "#These features or outputs from each encoder can be then passed on to the convlstm \n",
        "print(\"The outputs of each encoder in the embedded encoder:\")\n",
        "for ftr in encoderLayerOutputs: print(ftr.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF-rceP8dWAe",
        "outputId": "ff9e1098-6011-48b3-ddf0-296d64c72c75"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input is of the size torch.Size([20, 1, 64, 64])\n",
            "The outputs of each encoder in the embedded encoder:\n",
            "torch.Size([20, 64, 16, 16])\n",
            "torch.Size([20, 128, 8, 8])\n",
            "torch.Size([20, 256, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ModelDecoder=Embedded_Decoder()\n",
        "print(\"The input of the embedded decoder is the output from the last encoder in the embedded encoder\",encoderLayerOutputs[-1].shape)\n",
        "decoderLayerOutputs=ModelDecoder(encoderLayerOutputs[-1])\n",
        "print(\"The outputs of each encoder in the embedded decoder:\")\n",
        "for ftr in decoderLayerOutputs: print(ftr.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9snVORyWdF29",
        "outputId": "9a53acc3-44ae-4e20-e992-e1ed0644b3d3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input of the embedded decoder is the output from the last encoder in the embedded encoder torch.Size([20, 256, 4, 4])\n",
            "The outputs of each encoder in the embedded decoder:\n",
            "torch.Size([20, 128, 8, 8])\n",
            "torch.Size([20, 64, 16, 16])\n",
            "torch.Size([20, 1, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, bias,mode=\"zeros\"):\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
        "        self.bias = bias\n",
        "        self.mode = mode\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
        "                              out_channels=4 * self.hidden_dim,\n",
        "                              kernel_size=self.kernel_size,\n",
        "                              padding=self.padding,\n",
        "                              bias=self.bias)\n",
        "        \n",
        "       \n",
        "        \n",
        "\n",
        "    def forward(self, x, cur_state):\n",
        "        h_cur, c_cur = cur_state\n",
        "        x = x.to(device)\n",
        "        h_cur = h_cur.to(device)\n",
        "        # print(x.size())\n",
        "        # print(h_cur.size())\n",
        "        concat_input_hcur = torch.cat([x, h_cur], dim=1) \n",
        "        concat_input_hcur = concat_input_hcur.to(device)\n",
        "\n",
        "        concat_input_hcur_conv = self.conv(concat_input_hcur)\n",
        "        concat_input_hcur_conv = concat_input_hcur_conv.to(device)\n",
        "\n",
        "        cc_input_gate, cc_forget_gate, cc_output_gate, cc_output = torch.split(concat_input_hcur_conv, self.hidden_dim, dim=1)\n",
        "        # print(\"cci\",cc_input_gate.shape)\n",
        "        # print(\"ccf\",cc_forget_gate.shape)\n",
        "        # print(\"ccog\",cc_output_gate.shape)\n",
        "        # print(\"cco\",cc_output.shape)\n",
        "        # print(\"cccur\",c_cur.shape)\n",
        "        # print(\"wci\",self.W_ci.shape)\n",
        "        \n",
        "        input_gate = torch.sigmoid(cc_input_gate +  c_cur)\n",
        "\n",
        "        forget_gate = torch.sigmoid(cc_forget_gate +  c_cur)\n",
        "\n",
        "        output = torch.tanh(cc_output)\n",
        "\n",
        "        c_next = forget_gate * c_cur + input_gate * output\n",
        "\n",
        "        output_gate = torch.sigmoid(cc_output_gate +  c_next)\n",
        "\n",
        "        h_next = output * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_state(self, batch_size, image_size):\n",
        "        height, width = image_size\n",
        "        \"\"\" Initializing hidden and cell state \"\"\"\n",
        "        if(self.mode == \"zeros\"):\n",
        "            h = torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
        "            c = torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
        "        elif(self.mode == \"random\"):\n",
        "            h = torch.randn(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
        "            c = torch.randn(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)\n",
        "        elif(self.mode == \"learned\"):\n",
        "            h = self.learned_h.repeat(batch_size, 1, height, width, device=self.conv.weight.device)\n",
        "            c = self.learned_c.repeat(batch_size, 1, height, width, device=self.conv.weight.device)\n",
        "        \n",
        "        return h, c\n",
        "\n",
        "        \n",
        "\n",
        "class ConvLSTM(nn.Module):\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "    \"\"\" \n",
        "    Custom LSTM for images. Batches of images are fed to a Conv LSTM\n",
        "    \n",
        "    Args:\n",
        "    -----\n",
        "    input_dim: integer\n",
        "        Number of channels of the input.\n",
        "    hidden_dim: integer\n",
        "        dimensionality of the states in the cell\n",
        "    kernel_size: tuple\n",
        "        size of the kernel for convolutions\n",
        "    num_layers: integer\n",
        "        number of stacked LSTMS\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,batch_first=False, bias=True, return_all_layers=False):\n",
        "        super(ConvLSTM, self).__init__()\n",
        "\n",
        "        \n",
        "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
        "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
        "       \n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.bias = bias\n",
        "        self.return_all_layers = return_all_layers\n",
        "        classifier_in_dim= (self.hidden_dim[0]*16*16)\n",
        "\n",
        "        \n",
        "\n",
        "        conv_lstms  = []\n",
        "        # iterating over no of layers\n",
        "        for i in range(0, self.num_layers):\n",
        "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
        "\n",
        "            conv_lstms.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
        "                                          hidden_dim=self.hidden_dim[i],\n",
        "                                          kernel_size=self.kernel_size[i],\n",
        "                                          bias=self.bias))\n",
        "\n",
        "        self.conv_lstms = nn.ModuleList(conv_lstms)\n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "       \n",
        "\n",
        "        x=x.unsqueeze(dim=1)\n",
        "        b, _, _, h, w = x.size()\n",
        "\n",
        "        if hidden_state is not None:\n",
        "            raise NotImplementedError()\n",
        "        else:\n",
        "            hidden_state = self._init_hidden(batch_size=b,\n",
        "                                             image_size=(h, w))\n",
        "\n",
        "        \n",
        "        cur_layer_input = x\n",
        "        output_list = []\n",
        "        x_len = x.size(1)\n",
        "        \n",
        "\n",
        "        # iterating over no of layers\n",
        "        for i in range(self.num_layers):\n",
        "\n",
        "            h, c = hidden_state[i]\n",
        "            each_layer_output = []\n",
        "            # iterating over sequence length\n",
        "\n",
        "            for t in range(x_len):\n",
        "                h, c = self.conv_lstms[i](x=cur_layer_input[:, t, :, :, :],cur_state=[h, c])\n",
        "                each_layer_output.append(h)\n",
        "\n",
        "            stacked_layer_output = torch.stack(each_layer_output, dim=1)\n",
        "            cur_layer_input = stacked_layer_output\n",
        "\n",
        "            output_list.append(stacked_layer_output)\n",
        "\n",
        "        if not self.return_all_layers:\n",
        "            output_list = output_list[-1:]\n",
        "\n",
        "        batch_shape = output_list[-1].shape[0]\n",
        "\n",
        "        return torch.stack(each_layer_output), (h, c)\n",
        "\n",
        "    def _init_hidden(self, batch_size, image_size):\n",
        "        init_states = []\n",
        "        for i in range(self.num_layers):\n",
        "            init_states.append(self.conv_lstms[i].init_state(batch_size, image_size))\n",
        "        return init_states\n",
        "\n",
        "    @staticmethod\n",
        "    def _extend_for_multilayer(param, num_layers):\n",
        "        if not isinstance(param, list):\n",
        "            param = [param] * num_layers\n",
        "        return param"
      ],
      "metadata": {
        "id": "URrnzDLRm5l_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_model_FirstEncoder= ConvLSTM(input_dim= 64, hidden_dim = 64, kernel_size = (5,5), num_layers= 1)\n",
        "if torch.cuda.is_available():\n",
        "    conv_model_FirstEncoder.to(device)"
      ],
      "metadata": {
        "id": "Y0jJdJC3nXBQ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_model_SecondEncoder= ConvLSTM(input_dim= 128, hidden_dim = 128, kernel_size = (5,5), num_layers= 1)\n",
        "if torch.cuda.is_available():\n",
        "    conv_model_SecondEncoder.to(device)"
      ],
      "metadata": {
        "id": "4cbQW7Oxmylw"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_model_ThirdEncoder= ConvLSTM(input_dim= 256, hidden_dim = 256, kernel_size = (5,5), num_layers= 1)\n",
        "if torch.cuda.is_available():\n",
        "    conv_model_ThirdEncoder.to(device)"
      ],
      "metadata": {
        "id": "f-Bg4O98nAhq"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pMTmv2H6nJxe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}